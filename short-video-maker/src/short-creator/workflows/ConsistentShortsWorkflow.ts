import fs from "fs-extra";
import path from "path";
import { BaseWorkflow, WorkflowContext, WorkflowResult } from "./BaseWorkflow";
import { VideoProcessor } from "../processors/VideoProcessor";
import { GoogleVeoAPI } from "../libraries/GoogleVeo";
import { VIDEO_DIMENSIONS } from "../utils/Constants";
import { logger } from "../../logger";
import { ImageGenerationService } from "../../image-generation/services/ImageGenerationService";
import { ImageModelType } from "../../image-generation/models/imageModels";
import type { Scene, SceneInput } from "../../types/shorts";

/**
 * Minimum scene duration in seconds.
 * VEO3 generates 6-second videos minimum, so we use 5 seconds to ensure
 * good video content even when TTS audio is shorter.
 *
 * Scene duration = Math.max(audioDuration, MIN_SCENE_DURATION)
 * - If audio is 1.5s â†’ scene plays for 5s (audio at start, video continues)
 * - If audio is 7s â†’ scene plays for 7s (audio matches video)
 */
const MIN_SCENE_DURATION = 5;

/**
 * Consistent Shorts Workflow
 *
 * Inspired by Image_out.ipynb Chat Mode:
 * - Generates images with CHARACTER CONSISTENCY
 * - Uses previous images as references (max 3)
 * - Optional VEO3 I2V conversion
 * - Perfect for storytelling with same character
 *
 * How it works (like Chat Mode in ipynb):
 * Scene 1: Generate character image (no references)
 * Scene 2: Generate with Scene 1 as reference â†’ same character!
 * Scene 3: Generate with Scene 1, 2 as references â†’ same character!
 * Scene 4: Generate with Scene 2, 3 as references (max 3) â†’ same character!
 */
export class ConsistentShortsWorkflow extends BaseWorkflow {
  constructor(
    private videoProcessor: VideoProcessor,
    private imageGenerationService?: ImageGenerationService,
    private veoAPI?: GoogleVeoAPI
  ) {
    super();
  }

  /**
   * Validate scenes for Consistent Shorts workflow
   * Unlike base validation, we only require audio since we generate our own images/videos
   */
  private validateConsistentShortsScenes(scenes: Scene[]): void {
    if (!scenes || scenes.length === 0) {
      throw new Error("No scenes provided for processing");
    }

    for (let i = 0; i < scenes.length; i++) {
      const scene = scenes[i];
      // Only check for audio - video will be generated by this workflow
      if (!scene.audio) {
        throw new Error(`Scene ${i + 1} is missing audio`);
      }
    }
  }

  async process(
    scenes: Scene[],
    inputScenes: SceneInput[],
    context: WorkflowContext
  ): Promise<WorkflowResult> {
    try {
      logger.info({
        videoId: context.videoId,
        sceneCount: scenes.length,
        mode: "consistent-shorts"
      }, "âœ¨ Processing CONSISTENT SHORTS workflow (character consistency mode)");

      // Custom validation for Consistent Shorts: only require audio (we generate our own images/videos)
      this.validateConsistentShortsScenes(scenes);

      if (!this.imageGenerationService) {
        throw new Error("ImageGenerationService is required for Consistent Shorts mode");
      }

      // Create video-specific temp folder
      const videoTempDir = this.videoProcessor.createVideoTempDir(context.videoId);
      await fs.ensureDir(videoTempDir);
      logger.info({ videoTempDir, videoId: context.videoId }, "âœ… Created video-specific temp directory");

      const folderExists = await fs.pathExists(videoTempDir);
      if (!folderExists) {
        throw new Error(`Failed to create video temp directory: ${videoTempDir}`);
      }

      try {
        // Step 1: Generate images with CHARACTER CONSISTENCY
        logger.info({
          videoId: context.videoId,
          sceneCount: inputScenes.length
        }, "ğŸ¨ Starting CONSISTENT image generation (like Chat Mode in ipynb)");

        const imageDataList: Array<{
          imagePath: string;
          duration: number;
          sceneText: string;
          imageBuffer: Buffer;
        }> = [];

        // Track previous images for reference (like Chat history)
        const previousImages: Array<{
          data: Buffer;
          mimeType: string;
          sceneIndex: number;
        }> = [];

        for (let i = 0; i < inputScenes.length; i++) {
          const scene = inputScenes[i];

          logger.info({
            sceneIndex: i + 1,
            totalScenes: inputScenes.length,
            hasPreviousImages: previousImages.length > 0,
            referenceImageCount: Math.min(previousImages.length, 3)
          }, "ğŸ“¸ Generating image for scene with character consistency");

          if (!scene.imageData) {
            scene.imageData = {
              prompt: scene.text,
              style: "cinematic",
              mood: "dynamic"
            };
          }

          // Set NANO BANANA model (best for character consistency)
          this.imageGenerationService.setModel(ImageModelType.NANO_BANANA);

          // Enhanced prompt with character consistency
          const enhancedPrompt = `${scene.imageData.prompt || scene.text}. Style: ${scene.imageData.style || "cinematic"}. Mood: ${scene.imageData.mood || "dynamic"}. Maintain consistent character appearance.`;
          const aspectRatio = context.orientation === "portrait" ? "9:16" : "16:9";

          // â­ KEY FEATURE: Use previous images as references (max 3)
          // This is like Chat Mode in ipynb - maintains character consistency!
          const referenceImages = i > 0
            ? previousImages.slice(-3).map(img => ({
                data: img.data,
                mimeType: img.mimeType
              }))
            : undefined;

          logger.debug({
            sceneIndex: i,
            referenceImageCount: referenceImages?.length || 0,
            prompt: enhancedPrompt.substring(0, 100)
          }, "ğŸ”— Using reference images for consistency");

          // Generate image with references
          const result = await this.imageGenerationService.generateImages({
            prompt: enhancedPrompt,
            numberOfImages: 1,
            aspectRatio: aspectRatio as "9:16" | "16:9",
            referenceImages: referenceImages // â­ Chat Mode magic!
          }, context.videoId, i);

          if (!result.success || !result.images || result.images.length === 0) {
            throw new Error(`Failed to generate consistent image for scene ${i + 1}`);
          }

          const generatedImage = result.images[0];

          // Save image
          const simpleFilename = `consistent_scene_${i + 1}_${context.videoId}.png`;
          const savedImagePath = path.join(videoTempDir, simpleFilename);

          await fs.writeFile(savedImagePath, generatedImage.data);

          // Verify save
          const fileExists = await fs.pathExists(savedImagePath);
          const fileStats = fileExists ? await fs.stat(savedImagePath) : null;

          logger.info({
            sceneIndex: i + 1,
            imagePath: savedImagePath,
            filename: simpleFilename,
            fileExists,
            fileSize: fileStats?.size,
            usedReferences: referenceImages?.length || 0
          }, "âœ… Consistent character image generated and saved");

          // â­ Add to previous images for next scene reference
          previousImages.push({
            data: generatedImage.data,
            mimeType: generatedImage.mimeType || "image/png",
            sceneIndex: i
          });

          imageDataList.push({
            imagePath: savedImagePath,
            duration: 3, // Will be updated with actual audio length
            sceneText: scene.text,
            imageBuffer: generatedImage.data
          });
        }

        logger.info({
          totalImages: imageDataList.length,
          characterConsistent: true
        }, "ğŸ‰ All images generated with consistent character!");

        // Step 2: Update durations from audio data (with minimum scene duration)
        for (let i = 0; i < scenes.length; i++) {
          const scene = scenes[i];
          if (scene.audio?.duration) {
            // ìµœì†Œ ì”¬ ê¸¸ì´ ë³´ì¥: TTSê°€ ì§§ì•„ë„ ì¶©ë¶„í•œ ì½˜í…ì¸  ì œê³µ
            imageDataList[i].duration = Math.max(scene.audio.duration, MIN_SCENE_DURATION);
          }
        }

        // Step 3A: VEO3 I2V conversion (if enabled)
        logger.info({
          hasGenerateVideosFlag: !!context.metadata?.generateVideos,
          generateVideosValue: context.metadata?.generateVideos,
          hasVeoAPI: !!this.veoAPI,
          willUseVEO3: !!(context.metadata?.generateVideos && this.veoAPI)
        }, "ğŸ” Checking VEO3 I2V condition");

        if (context.metadata?.generateVideos && this.veoAPI) {
          logger.info("ğŸ¬ Converting consistent images to videos with VEO3 I2V");

          // Track which scenes use VEO3 video vs fallback image
          const sceneResults: Array<{
            type: 'video' | 'image';
            path: string;
            duration: number;
          }> = [];

          let veo3SuccessCount = 0;
          let veo3FailCount = 0;

          for (let i = 0; i < imageDataList.length; i++) {
            const imageData = imageDataList[i];
            const scene = inputScenes[i];
            const duration = imageData.duration || scenes[i]?.audio?.duration || 8;

            logger.info({
              sceneIndex: i + 1,
              duration
            }, "ğŸ”„ Converting image to video with VEO3");

            try {
              // Convert image to base64 for VEO3
              const imageBase64 = imageData.imageBuffer.toString('base64');

              // VEO3 I2V generation
              const videoPrompt = scene.videoPrompt || scene.text || `Scene ${i + 1}`;

              const video = await this.veoAPI.findVideo(
                [videoPrompt],
                duration,          // minDurationSeconds (number)
                [],                // excludeIds
                context.orientation, // orientation
                300000,            // timeout (5 minutes)
                0,                 // retryCounter
                {                  // initialImage for I2V
                  data: imageBase64,
                  mimeType: "image/png"
                }
              );

              // Download VEO3 video
              const videoPath = path.join(videoTempDir, `veo3_scene_${i + 1}_${context.videoId}.mp4`);
              await this.videoProcessor.downloadVideo(video.url, videoPath);

              sceneResults.push({
                type: 'video',
                path: videoPath,
                duration
              });

              veo3SuccessCount++;

              logger.info({
                sceneIndex: i + 1,
                videoPath
              }, "âœ… VEO3 video generated from consistent image");

            } catch (veoError) {
              // VEO3 ì‹¤íŒ¨ â†’ ì´ë¯¸ì§€ë¡œ fallback
              veo3FailCount++;

              logger.warn({
                sceneIndex: i + 1,
                error: veoError instanceof Error ? veoError.message : 'Unknown error',
                totalFailed: veo3FailCount
              }, "âš ï¸ VEO3 failed for scene, falling back to static image");

              sceneResults.push({
                type: 'image',
                path: imageData.imagePath,
                duration
              });
            }
          }

          logger.info({
            totalScenes: imageDataList.length,
            veo3Success: veo3SuccessCount,
            veo3Failed: veo3FailCount,
            fallbackUsed: veo3FailCount > 0
          }, "ğŸ“Š VEO3 conversion summary");

          // í˜¼í•© ì²˜ë¦¬: VEO3 ë¹„ë””ì˜¤ + fallback ì´ë¯¸ì§€ ê²°í•©
          let tempVideoPath: string;

          if (veo3FailCount === 0) {
            // ëª¨ë“  scene VEO3 ì„±ê³µ â†’ ê° ë¹„ë””ì˜¤ë¥¼ ì ì ˆí•œ ê¸¸ì´ë¡œ íŠ¸ë¦¬ë° í›„ ê²°í•©
            // VEO3ëŠ” ìµœì†Œ 6ì´ˆ ë¹„ë””ì˜¤ë¥¼ ìƒì„±í•˜ë¯€ë¡œ, ìµœì†Œ ì”¬ ê¸¸ì´ ë³´ì¥ í•„ìš”
            const trimmedVideoPaths: string[] = [];

            for (let i = 0; i < sceneResults.length; i++) {
              const result = sceneResults[i];
              const audioDuration = scenes[i]?.audio?.duration || result.duration;
              // ìµœì†Œ ì”¬ ê¸¸ì´ ë³´ì¥: TTSê°€ ì§§ì•„ë„ VEO3 ì½˜í…ì¸  í™œìš©
              const sceneDuration = Math.max(audioDuration, MIN_SCENE_DURATION);

              const trimmedPath = path.join(videoTempDir, `trimmed_scene_${i + 1}_${context.videoId}.mp4`);

              logger.info({
                sceneIndex: i + 1,
                originalPath: result.path,
                audioDuration,
                sceneDuration,
                minSceneDuration: MIN_SCENE_DURATION,
                trimmedPath
              }, "âœ‚ï¸ Trimming VEO3 video (respecting min scene duration)");

              await this.videoProcessor.trimVideo(result.path, trimmedPath, sceneDuration);
              trimmedVideoPaths.push(trimmedPath);
            }

            logger.info({
              clipCount: trimmedVideoPaths.length,
              clips: trimmedVideoPaths
            }, "ğŸ¬ Combining trimmed VEO3 video clips");

            tempVideoPath = path.join(videoTempDir, `veo3_combined_${context.videoId}.mp4`);
            await this.videoProcessor.combineVideoClips(trimmedVideoPaths, tempVideoPath);

          } else if (veo3SuccessCount === 0) {
            // ëª¨ë“  scene VEO3 ì‹¤íŒ¨ â†’ ì •ì  ì´ë¯¸ì§€ ë¹„ë””ì˜¤
            logger.info("âš ï¸ All VEO3 failed, creating static video from images");

            const dimensions = context.orientation === "portrait"
              ? VIDEO_DIMENSIONS.PORTRAIT
              : VIDEO_DIMENSIONS.LANDSCAPE;

            tempVideoPath = path.join(videoTempDir, `fallback_static_${context.videoId}.mp4`);
            await this.videoProcessor.createStaticVideoFromMultipleImages(
              imageDataList,
              tempVideoPath,
              dimensions
            );

          } else {
            // í˜¼í•©: ì¼ë¶€ ì„±ê³µ, ì¼ë¶€ ì‹¤íŒ¨ â†’ VEO3 ë¹„ë””ì˜¤ íŠ¸ë¦¬ë° + ì‹¤íŒ¨í•œ ê²ƒì€ ì´ë¯¸ì§€ë¡œ
            logger.info({
              successCount: veo3SuccessCount,
              failCount: veo3FailCount
            }, "ğŸ”€ Mixed results: combining VEO3 videos with static images");

            const dimensions = context.orientation === "portrait"
              ? VIDEO_DIMENSIONS.PORTRAIT
              : VIDEO_DIMENSIONS.LANDSCAPE;

            const processedClips: string[] = [];

            for (let i = 0; i < sceneResults.length; i++) {
              const result = sceneResults[i];
              const audioDuration = scenes[i]?.audio?.duration || result.duration;
              // ìµœì†Œ ì”¬ ê¸¸ì´ ë³´ì¥: TTSê°€ ì§§ì•„ë„ ì¶©ë¶„í•œ ì½˜í…ì¸  ì œê³µ
              const sceneDuration = Math.max(audioDuration, MIN_SCENE_DURATION);

              if (result.type === 'video') {
                // VEO3 ì„±ê³µ â†’ ìµœì†Œ ì”¬ ê¸¸ì´ ë³´ì¥í•˜ì—¬ íŠ¸ë¦¬ë°
                const trimmedPath = path.join(videoTempDir, `trimmed_mixed_${i + 1}_${context.videoId}.mp4`);
                await this.videoProcessor.trimVideo(result.path, trimmedPath, sceneDuration);
                processedClips.push(trimmedPath);
              } else {
                // VEO3 ì‹¤íŒ¨ â†’ ì´ë¯¸ì§€ë¡œ ë¹„ë””ì˜¤ ìƒì„± (ìµœì†Œ ì”¬ ê¸¸ì´ ì ìš©)
                const imageVideoPath = path.join(videoTempDir, `image_to_video_${i + 1}_${context.videoId}.mp4`);
                await this.videoProcessor.createStaticVideoFromMultipleImages(
                  [{ imagePath: result.path, duration: sceneDuration }],
                  imageVideoPath,
                  dimensions
                );
                processedClips.push(imageVideoPath);
              }
            }

            // ëª¨ë“  ì²˜ë¦¬ëœ í´ë¦½ ê²°í•©
            tempVideoPath = path.join(videoTempDir, `mixed_combined_${context.videoId}.mp4`);
            await this.videoProcessor.combineVideoClips(processedClips, tempVideoPath);
          }

          logger.info({
            clipCount: sceneResults.length,
            outputPath: tempVideoPath
          }, "âœ… Video clips combined");

          // Step 3B: Combine with audio
          const audioFiles: string[] = [];
          for (const scene of scenes) {
            if (scene.audio?.url) {
              const audioFileName = scene.audio.url.split('/').pop();
              if (audioFileName) {
                audioFiles.push(path.join(this.videoProcessor.getConfig().tempDirPath, audioFileName));
              }
            }
          }

          const tempFinalPath = path.join(videoTempDir, `final_${context.videoId}.mp4`);
          await this.videoProcessor.combineVideoWithAudio(
            tempVideoPath,
            audioFiles,
            tempFinalPath
          );

          // Copy final video to standard location for GCS upload
          const standardVideoPath = path.join(
            this.videoProcessor.getConfig().videosDirPath,
            `${context.videoId}.mp4`
          );

          await fs.promises.copyFile(tempFinalPath, standardVideoPath);
          logger.info({
            from: tempFinalPath,
            to: standardVideoPath
          }, "âœ… Final video copied to standard location for GCS upload");

          // Calculate total duration
          const totalDuration = this.calculateTotalDuration(scenes);

          return {
            outputPath: standardVideoPath,
            duration: totalDuration,
            scenes
          };

        } else {
          // Step 3B: Static video from images (no VEO3)
          logger.info("ğŸï¸ Creating static video from consistent images");

          const tempVideoPath = path.join(videoTempDir, `consistent_static_${context.videoId}.mp4`);
          const dimensions = context.orientation === "portrait"
            ? VIDEO_DIMENSIONS.PORTRAIT
            : VIDEO_DIMENSIONS.LANDSCAPE;

          await this.videoProcessor.createStaticVideoFromMultipleImages(
            imageDataList,
            tempVideoPath,
            dimensions
          );

          logger.info("âœ… Static video created from consistent character images");

          // Step 4: Combine with audio
          const audioFiles: string[] = [];
          for (const scene of scenes) {
            if (scene.audio?.url) {
              const audioFileName = scene.audio.url.split('/').pop();
              if (audioFileName) {
                audioFiles.push(path.join(this.videoProcessor.getConfig().tempDirPath, audioFileName));
              }
            }
          }

          const tempFinalPath = path.join(videoTempDir, `final_${context.videoId}.mp4`);
          await this.videoProcessor.combineVideoWithAudio(
            tempVideoPath,
            audioFiles,
            tempFinalPath
          );

          // Copy final video to standard location for GCS upload
          const standardVideoPath = path.join(
            this.videoProcessor.getConfig().videosDirPath,
            `${context.videoId}.mp4`
          );

          await fs.promises.copyFile(tempFinalPath, standardVideoPath);
          logger.info({
            from: tempFinalPath,
            to: standardVideoPath
          }, "âœ… Final video copied to standard location for GCS upload");

          // Calculate total duration
          const totalDuration = this.calculateTotalDuration(scenes);

          return {
            outputPath: standardVideoPath,
            duration: totalDuration,
            scenes
          };
        }

      } catch (error) {
        logger.error({ error, videoId: context.videoId }, "âŒ Consistent Shorts workflow failed");
        throw error;
      }

    } catch (error) {
      logger.error({ error }, "Failed to process Consistent Shorts workflow");
      throw error;
    }
  }
}
