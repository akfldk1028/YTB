import fs from "fs-extra";
import path from "path";
import { BaseWorkflow, WorkflowContext, WorkflowResult } from "./BaseWorkflow";
import { VideoProcessor } from "../processors/VideoProcessor";
import { GoogleVeoAPI } from "../libraries/GoogleVeo";
import { VIDEO_DIMENSIONS } from "../utils/Constants";
import { logger } from "../../logger";
import { ImageGenerationService } from "../../image-generation/services/ImageGenerationService";
import { ImageModelType } from "../../image-generation/models/imageModels";
import { CharacterStorageService } from "../../character-store/CharacterStorageService";
import type { CharacterProfile, Character } from "../../character-store/types";
import type { Scene, SceneInput } from "../../types/shorts";

/**
 * Minimum scene duration in seconds.
 * VEO3 generates 6-second videos minimum, so we use 5 seconds to ensure
 * good video content even when TTS audio is shorter.
 *
 * Scene duration = Math.max(audioDuration, MIN_SCENE_DURATION)
 * - If audio is 1.5s ‚Üí scene plays for 5s (audio at start, video continues)
 * - If audio is 7s ‚Üí scene plays for 7s (audio matches video)
 */
const MIN_SCENE_DURATION = 5;

/**
 * Consistent Shorts Workflow
 *
 * Inspired by Image_out.ipynb Chat Mode:
 * - Generates images with CHARACTER CONSISTENCY
 * - Uses previous images as references (max 3)
 * - Optional VEO3 I2V conversion
 * - Perfect for storytelling with same character
 *
 * How it works (like Chat Mode in ipynb):
 * Scene 1: Generate character image (no references)
 * Scene 2: Generate with Scene 1 as reference ‚Üí same character!
 * Scene 3: Generate with Scene 1, 2 as references ‚Üí same character!
 * Scene 4: Generate with Scene 2, 3 as references (max 3) ‚Üí same character!
 */
export class ConsistentShortsWorkflow extends BaseWorkflow {
  constructor(
    private videoProcessor: VideoProcessor,
    private imageGenerationService?: ImageGenerationService,
    private veoAPI?: GoogleVeoAPI,
    private characterStorage?: CharacterStorageService
  ) {
    super();
  }

  /**
   * Load stored character reference images from GCS
   * These images serve as the starting point for character consistency
   */
  private async loadStoredCharacterImages(
    profileId: string,
    characterIds?: string[]
  ): Promise<Array<{ data: Buffer; mimeType: string; characterId: string; description: string }>> {
    if (!this.characterStorage || !this.characterStorage.isEnabled()) {
      logger.warn("CharacterStorageService not available, skipping stored images");
      return [];
    }

    try {
      // Load profile
      const profileResult = await this.characterStorage.getProfile(profileId);
      if (!profileResult.success || !profileResult.data) {
        logger.warn({ profileId }, "Character profile not found");
        return [];
      }

      const profile = profileResult.data;

      // Load character images
      const imagesResult = await this.characterStorage.loadCharacterImages(profileId);
      if (!imagesResult.success || !imagesResult.data) {
        logger.warn({ profileId }, "No character images found");
        return [];
      }

      const storedImages: Array<{ data: Buffer; mimeType: string; characterId: string; description: string }> = [];

      // Filter by characterIds if provided
      const targetCharacters = characterIds
        ? profile.characters.filter(c => characterIds.includes(c.id))
        : profile.characters;

      for (const character of targetCharacters) {
        const imageBase64 = imagesResult.data.get(character.id);
        if (imageBase64) {
          storedImages.push({
            data: Buffer.from(imageBase64, 'base64'),
            mimeType: 'image/png',
            characterId: character.id,
            description: character.description
          });
        }
      }

      logger.info({
        profileId,
        loadedCharacters: storedImages.length,
        characterIds: storedImages.map(c => c.characterId)
      }, "‚úÖ Loaded stored character reference images");

      return storedImages;

    } catch (error) {
      logger.error({ error, profileId }, "Failed to load stored character images");
      return [];
    }
  }

  /**
   * Build character description from stored profile
   */
  private buildCharacterDescription(profile: CharacterProfile, characters: Character[]): string {
    const descriptions = characters.map(c => {
      let desc = c.description;
      if (c.distinguishingFeatures) {
        desc += `. Distinguishing features: ${c.distinguishingFeatures}`;
      }
      return `${c.name}: ${desc}`;
    });

    let fullDescription = descriptions.join('\n');
    if (profile.defaultStyle) {
      fullDescription += `\nStyle: ${profile.defaultStyle}`;
    }
    if (profile.defaultMood) {
      fullDescription += `\nMood: ${profile.defaultMood}`;
    }

    return fullDescription;
  }

  /**
   * Validate scenes for Consistent Shorts workflow
   * Unlike base validation, we only require audio since we generate our own images/videos
   */
  private validateConsistentShortsScenes(scenes: Scene[]): void {
    if (!scenes || scenes.length === 0) {
      throw new Error("No scenes provided for processing");
    }

    for (let i = 0; i < scenes.length; i++) {
      const scene = scenes[i];
      // Only check for audio - video will be generated by this workflow
      if (!scene.audio) {
        throw new Error(`Scene ${i + 1} is missing audio`);
      }
    }
  }

  async process(
    scenes: Scene[],
    inputScenes: SceneInput[],
    context: WorkflowContext
  ): Promise<WorkflowResult> {
    try {
      // üîç DEBUG: Log full context.metadata to understand what's being passed
      logger.info({
        videoId: context.videoId,
        sceneCount: scenes.length,
        mode: "consistent-shorts",
        hasMetadata: !!context.metadata,
        metadataKeys: context.metadata ? Object.keys(context.metadata) : [],
        characterProfileId: context.metadata?.characterProfileId,
        characterIds: context.metadata?.characterIds,
        fullMetadata: JSON.stringify(context.metadata || {}).substring(0, 500)
      }, "‚ú® Processing CONSISTENT SHORTS workflow - DEBUG metadata");

      // Custom validation for Consistent Shorts: only require audio (we generate our own images/videos)
      this.validateConsistentShortsScenes(scenes);

      if (!this.imageGenerationService) {
        throw new Error("ImageGenerationService is required for Consistent Shorts mode");
      }

      // Create video-specific temp folder
      const videoTempDir = this.videoProcessor.createVideoTempDir(context.videoId);
      await fs.ensureDir(videoTempDir);
      logger.info({ videoTempDir, videoId: context.videoId }, "‚úÖ Created video-specific temp directory");

      const folderExists = await fs.pathExists(videoTempDir);
      if (!folderExists) {
        throw new Error(`Failed to create video temp directory: ${videoTempDir}`);
      }

      try {
        // Step 1: Generate images with CHARACTER CONSISTENCY
        logger.info({
          videoId: context.videoId,
          sceneCount: inputScenes.length
        }, "üé® Starting CONSISTENT image generation (like Chat Mode in ipynb)");

        const imageDataList: Array<{
          imagePath: string;
          duration: number;
          sceneText: string;
          imageBuffer: Buffer;
        }> = [];

        // Track previous images for reference (like Chat history)
        const previousImages: Array<{
          data: Buffer;
          mimeType: string;
          sceneIndex: number;
        }> = [];

        // ‚≠ê NEW: Load stored character images if profileId is provided
        // This enables character persistence across multiple video sessions!
        const characterProfileId = context.metadata?.characterProfileId as string | undefined;
        const characterIds = context.metadata?.characterIds as string[] | undefined;

        // üîç DEBUG: Always log this check
        logger.info({
          characterProfileId,
          characterIds,
          hasCharacterStorage: !!this.characterStorage,
          characterStorageEnabled: this.characterStorage?.isEnabled?.() ?? false
        }, "üîç DEBUG: Checking if should load stored character images");

        if (characterProfileId) {
          logger.info({
            characterProfileId,
            characterIds
          }, "üé≠ Loading stored character reference images for consistency");

          const storedImages = await this.loadStoredCharacterImages(characterProfileId, characterIds);

          // Add stored images as initial references (index: -1 to -N)
          for (let idx = 0; idx < storedImages.length; idx++) {
            const stored = storedImages[idx];
            previousImages.push({
              data: stored.data,
              mimeType: stored.mimeType,
              sceneIndex: -(idx + 1) // Negative index for stored images
            });
          }

          logger.info({
            storedImageCount: storedImages.length,
            previousImagesTotal: previousImages.length
          }, "‚úÖ Stored character images loaded as initial references");
        }

        for (let i = 0; i < inputScenes.length; i++) {
          const scene = inputScenes[i];

          logger.info({
            sceneIndex: i + 1,
            totalScenes: inputScenes.length,
            hasPreviousImages: previousImages.length > 0,
            referenceImageCount: Math.min(previousImages.length, 3)
          }, "üì∏ Generating image for scene with character consistency");

          if (!scene.imageData) {
            scene.imageData = {
              prompt: scene.text,
              style: "cinematic",
              mood: "dynamic"
            };
          }

          // Set NANO BANANA model (best for character consistency)
          this.imageGenerationService.setModel(ImageModelType.NANO_BANANA);

          // Enhanced prompt with character consistency
          const enhancedPrompt = `${scene.imageData.prompt || scene.text}. Style: ${scene.imageData.style || "cinematic"}. Mood: ${scene.imageData.mood || "dynamic"}. Maintain consistent character appearance.`;
          const aspectRatio = context.orientation === "portrait" ? "9:16" : "16:9";

          // ‚≠ê KEY FEATURE: Use previous images as references (max 3)
          // This is like Chat Mode in ipynb - maintains character consistency!
          // ‚≠ê UPDATED: If we have stored character images, use them even for scene 0
          const referenceImages = previousImages.length > 0
            ? previousImages.slice(-3).map(img => ({
                data: img.data,
                mimeType: img.mimeType
              }))
            : undefined;

          logger.debug({
            sceneIndex: i,
            referenceImageCount: referenceImages?.length || 0,
            prompt: enhancedPrompt.substring(0, 100)
          }, "üîó Using reference images for consistency");

          // Generate image with references
          const result = await this.imageGenerationService.generateImages({
            prompt: enhancedPrompt,
            numberOfImages: 1,
            aspectRatio: aspectRatio as "9:16" | "16:9",
            referenceImages: referenceImages // ‚≠ê Chat Mode magic!
          }, context.videoId, i);

          if (!result.success || !result.images || result.images.length === 0) {
            throw new Error(`Failed to generate consistent image for scene ${i + 1}`);
          }

          const generatedImage = result.images[0];

          // Save image
          const simpleFilename = `consistent_scene_${i + 1}_${context.videoId}.png`;
          const savedImagePath = path.join(videoTempDir, simpleFilename);

          await fs.writeFile(savedImagePath, generatedImage.data);

          // Verify save
          const fileExists = await fs.pathExists(savedImagePath);
          const fileStats = fileExists ? await fs.stat(savedImagePath) : null;

          logger.info({
            sceneIndex: i + 1,
            imagePath: savedImagePath,
            filename: simpleFilename,
            fileExists,
            fileSize: fileStats?.size,
            usedReferences: referenceImages?.length || 0
          }, "‚úÖ Consistent character image generated and saved");

          // ‚≠ê Add to previous images for next scene reference
          previousImages.push({
            data: generatedImage.data,
            mimeType: generatedImage.mimeType || "image/png",
            sceneIndex: i
          });

          imageDataList.push({
            imagePath: savedImagePath,
            duration: 3, // Will be updated with actual audio length
            sceneText: scene.text,
            imageBuffer: generatedImage.data
          });
        }

        logger.info({
          totalImages: imageDataList.length,
          characterConsistent: true
        }, "üéâ All images generated with consistent character!");

        // Step 2: Update durations from audio data (with minimum scene duration)
        for (let i = 0; i < scenes.length; i++) {
          const scene = scenes[i];
          if (scene.audio?.duration) {
            // ÏµúÏÜå Ïî¨ Í∏∏Ïù¥ Î≥¥Ïû•: TTSÍ∞Ä ÏßßÏïÑÎèÑ Ï∂©Î∂ÑÌïú ÏΩòÌÖêÏ∏† Ï†úÍ≥µ
            imageDataList[i].duration = Math.max(scene.audio.duration, MIN_SCENE_DURATION);
          }
        }

        // Step 3A: VEO3 I2V conversion (if enabled)
        logger.info({
          hasGenerateVideosFlag: !!context.metadata?.generateVideos,
          generateVideosValue: context.metadata?.generateVideos,
          hasVeoAPI: !!this.veoAPI,
          willUseVEO3: !!(context.metadata?.generateVideos && this.veoAPI)
        }, "üîç Checking VEO3 I2V condition");

        if (context.metadata?.generateVideos && this.veoAPI) {
          logger.info("üé¨ Converting consistent images to videos with VEO3 I2V");

          // Track which scenes use VEO3 video vs fallback image
          const sceneResults: Array<{
            type: 'video' | 'image';
            path: string;
            duration: number;
          }> = [];

          let veo3SuccessCount = 0;
          let veo3FailCount = 0;

          for (let i = 0; i < imageDataList.length; i++) {
            const imageData = imageDataList[i];
            const scene = inputScenes[i];
            const duration = imageData.duration || scenes[i]?.audio?.duration || 8;

            logger.info({
              sceneIndex: i + 1,
              duration
            }, "üîÑ Converting image to video with VEO3");

            try {
              // Convert image to base64 for VEO3
              const imageBase64 = imageData.imageBuffer.toString('base64');

              // VEO3 I2V generation
              const videoPrompt = scene.videoPrompt || scene.text || `Scene ${i + 1}`;

              const video = await this.veoAPI.findVideo(
                [videoPrompt],
                duration,          // minDurationSeconds (number)
                [],                // excludeIds
                context.orientation, // orientation
                300000,            // timeout (5 minutes)
                0,                 // retryCounter
                {                  // initialImage for I2V
                  data: imageBase64,
                  mimeType: "image/png"
                }
              );

              // Download VEO3 video
              const videoPath = path.join(videoTempDir, `veo3_scene_${i + 1}_${context.videoId}.mp4`);
              await this.videoProcessor.downloadVideo(video.url, videoPath);

              sceneResults.push({
                type: 'video',
                path: videoPath,
                duration
              });

              veo3SuccessCount++;

              logger.info({
                sceneIndex: i + 1,
                videoPath
              }, "‚úÖ VEO3 video generated from consistent image");

            } catch (veoError) {
              // VEO3 Ïã§Ìå® ‚Üí Ïù¥ÎØ∏ÏßÄÎ°ú fallback
              veo3FailCount++;

              logger.warn({
                sceneIndex: i + 1,
                error: veoError instanceof Error ? veoError.message : 'Unknown error',
                totalFailed: veo3FailCount
              }, "‚ö†Ô∏è VEO3 failed for scene, falling back to static image");

              sceneResults.push({
                type: 'image',
                path: imageData.imagePath,
                duration
              });
            }
          }

          logger.info({
            totalScenes: imageDataList.length,
            veo3Success: veo3SuccessCount,
            veo3Failed: veo3FailCount,
            fallbackUsed: veo3FailCount > 0
          }, "üìä VEO3 conversion summary");

          // ÌòºÌï© Ï≤òÎ¶¨: VEO3 ÎπÑÎîîÏò§ + fallback Ïù¥ÎØ∏ÏßÄ Í≤∞Ìï©
          let tempVideoPath: string;

          if (veo3FailCount === 0) {
            // Î™®Îì† scene VEO3 ÏÑ±Í≥µ ‚Üí Í∞Å ÎπÑÎîîÏò§Î•º Ï†ÅÏ†àÌïú Í∏∏Ïù¥Î°ú Ìä∏Î¶¨Î∞ç ÌõÑ Í≤∞Ìï©
            // VEO3Îäî ÏµúÏÜå 6Ï¥à ÎπÑÎîîÏò§Î•º ÏÉùÏÑ±ÌïòÎØÄÎ°ú, ÏµúÏÜå Ïî¨ Í∏∏Ïù¥ Î≥¥Ïû• ÌïÑÏöî
            const trimmedVideoPaths: string[] = [];

            for (let i = 0; i < sceneResults.length; i++) {
              const result = sceneResults[i];
              const audioDuration = scenes[i]?.audio?.duration || result.duration;
              // ÏµúÏÜå Ïî¨ Í∏∏Ïù¥ Î≥¥Ïû•: TTSÍ∞Ä ÏßßÏïÑÎèÑ VEO3 ÏΩòÌÖêÏ∏† ÌôúÏö©
              const sceneDuration = Math.max(audioDuration, MIN_SCENE_DURATION);

              const trimmedPath = path.join(videoTempDir, `trimmed_scene_${i + 1}_${context.videoId}.mp4`);

              logger.info({
                sceneIndex: i + 1,
                originalPath: result.path,
                audioDuration,
                sceneDuration,
                minSceneDuration: MIN_SCENE_DURATION,
                trimmedPath
              }, "‚úÇÔ∏è Trimming VEO3 video (respecting min scene duration)");

              await this.videoProcessor.trimVideo(result.path, trimmedPath, sceneDuration);
              trimmedVideoPaths.push(trimmedPath);
            }

            logger.info({
              clipCount: trimmedVideoPaths.length,
              clips: trimmedVideoPaths
            }, "üé¨ Combining trimmed VEO3 video clips");

            tempVideoPath = path.join(videoTempDir, `veo3_combined_${context.videoId}.mp4`);
            await this.videoProcessor.combineVideoClips(trimmedVideoPaths, tempVideoPath);

          } else if (veo3SuccessCount === 0) {
            // Î™®Îì† scene VEO3 Ïã§Ìå® ‚Üí Ï†ïÏ†Å Ïù¥ÎØ∏ÏßÄ ÎπÑÎîîÏò§
            logger.info("‚ö†Ô∏è All VEO3 failed, creating static video from images");

            const dimensions = context.orientation === "portrait"
              ? VIDEO_DIMENSIONS.PORTRAIT
              : VIDEO_DIMENSIONS.LANDSCAPE;

            tempVideoPath = path.join(videoTempDir, `fallback_static_${context.videoId}.mp4`);
            await this.videoProcessor.createStaticVideoFromMultipleImages(
              imageDataList,
              tempVideoPath,
              dimensions
            );

          } else {
            // ÌòºÌï©: ÏùºÎ∂Ä ÏÑ±Í≥µ, ÏùºÎ∂Ä Ïã§Ìå® ‚Üí VEO3 ÎπÑÎîîÏò§ Ìä∏Î¶¨Î∞ç + Ïã§Ìå®Ìïú Í≤ÉÏùÄ Ïù¥ÎØ∏ÏßÄÎ°ú
            logger.info({
              successCount: veo3SuccessCount,
              failCount: veo3FailCount
            }, "üîÄ Mixed results: combining VEO3 videos with static images");

            const dimensions = context.orientation === "portrait"
              ? VIDEO_DIMENSIONS.PORTRAIT
              : VIDEO_DIMENSIONS.LANDSCAPE;

            const processedClips: string[] = [];

            for (let i = 0; i < sceneResults.length; i++) {
              const result = sceneResults[i];
              const audioDuration = scenes[i]?.audio?.duration || result.duration;
              // ÏµúÏÜå Ïî¨ Í∏∏Ïù¥ Î≥¥Ïû•: TTSÍ∞Ä ÏßßÏïÑÎèÑ Ï∂©Î∂ÑÌïú ÏΩòÌÖêÏ∏† Ï†úÍ≥µ
              const sceneDuration = Math.max(audioDuration, MIN_SCENE_DURATION);

              if (result.type === 'video') {
                // VEO3 ÏÑ±Í≥µ ‚Üí ÏµúÏÜå Ïî¨ Í∏∏Ïù¥ Î≥¥Ïû•ÌïòÏó¨ Ìä∏Î¶¨Î∞ç
                const trimmedPath = path.join(videoTempDir, `trimmed_mixed_${i + 1}_${context.videoId}.mp4`);
                await this.videoProcessor.trimVideo(result.path, trimmedPath, sceneDuration);
                processedClips.push(trimmedPath);
              } else {
                // VEO3 Ïã§Ìå® ‚Üí Ïù¥ÎØ∏ÏßÄÎ°ú ÎπÑÎîîÏò§ ÏÉùÏÑ± (ÏµúÏÜå Ïî¨ Í∏∏Ïù¥ Ï†ÅÏö©)
                const imageVideoPath = path.join(videoTempDir, `image_to_video_${i + 1}_${context.videoId}.mp4`);
                await this.videoProcessor.createStaticVideoFromMultipleImages(
                  [{ imagePath: result.path, duration: sceneDuration }],
                  imageVideoPath,
                  dimensions
                );
                processedClips.push(imageVideoPath);
              }
            }

            // Î™®Îì† Ï≤òÎ¶¨Îêú ÌÅ¥Î¶Ω Í≤∞Ìï©
            tempVideoPath = path.join(videoTempDir, `mixed_combined_${context.videoId}.mp4`);
            await this.videoProcessor.combineVideoClips(processedClips, tempVideoPath);
          }

          logger.info({
            clipCount: sceneResults.length,
            outputPath: tempVideoPath
          }, "‚úÖ Video clips combined");

          // Step 3B: Combine with audio
          const audioFiles: string[] = [];
          for (const scene of scenes) {
            if (scene.audio?.url) {
              const audioFileName = scene.audio.url.split('/').pop();
              if (audioFileName) {
                audioFiles.push(path.join(this.videoProcessor.getConfig().tempDirPath, audioFileName));
              }
            }
          }

          const tempFinalPath = path.join(videoTempDir, `final_${context.videoId}.mp4`);
          await this.videoProcessor.combineVideoWithAudio(
            tempVideoPath,
            audioFiles,
            tempFinalPath
          );

          // Copy final video to standard location for GCS upload
          const standardVideoPath = path.join(
            this.videoProcessor.getConfig().videosDirPath,
            `${context.videoId}.mp4`
          );

          await fs.promises.copyFile(tempFinalPath, standardVideoPath);
          logger.info({
            from: tempFinalPath,
            to: standardVideoPath
          }, "‚úÖ Final video copied to standard location for GCS upload");

          // Calculate total duration
          const totalDuration = this.calculateTotalDuration(scenes);

          return {
            outputPath: standardVideoPath,
            duration: totalDuration,
            scenes
          };

        } else {
          // Step 3B: Static video from images (no VEO3)
          logger.info("üéûÔ∏è Creating static video from consistent images");

          const tempVideoPath = path.join(videoTempDir, `consistent_static_${context.videoId}.mp4`);
          const dimensions = context.orientation === "portrait"
            ? VIDEO_DIMENSIONS.PORTRAIT
            : VIDEO_DIMENSIONS.LANDSCAPE;

          await this.videoProcessor.createStaticVideoFromMultipleImages(
            imageDataList,
            tempVideoPath,
            dimensions
          );

          logger.info("‚úÖ Static video created from consistent character images");

          // Step 4: Combine with audio
          const audioFiles: string[] = [];
          for (const scene of scenes) {
            if (scene.audio?.url) {
              const audioFileName = scene.audio.url.split('/').pop();
              if (audioFileName) {
                audioFiles.push(path.join(this.videoProcessor.getConfig().tempDirPath, audioFileName));
              }
            }
          }

          const tempFinalPath = path.join(videoTempDir, `final_${context.videoId}.mp4`);
          await this.videoProcessor.combineVideoWithAudio(
            tempVideoPath,
            audioFiles,
            tempFinalPath
          );

          // Copy final video to standard location for GCS upload
          const standardVideoPath = path.join(
            this.videoProcessor.getConfig().videosDirPath,
            `${context.videoId}.mp4`
          );

          await fs.promises.copyFile(tempFinalPath, standardVideoPath);
          logger.info({
            from: tempFinalPath,
            to: standardVideoPath
          }, "‚úÖ Final video copied to standard location for GCS upload");

          // Calculate total duration
          const totalDuration = this.calculateTotalDuration(scenes);

          return {
            outputPath: standardVideoPath,
            duration: totalDuration,
            scenes
          };
        }

      } catch (error) {
        logger.error({ error, videoId: context.videoId }, "‚ùå Consistent Shorts workflow failed");
        throw error;
      }

    } catch (error) {
      logger.error({ error }, "Failed to process Consistent Shorts workflow");
      throw error;
    }
  }
}
